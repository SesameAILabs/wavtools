(()=>{var m=class{static floatTo16BitPCM(e){let a=new ArrayBuffer(e.length*2),t=new DataView(a),s=0;for(let r=0;r<e.length;r++,s+=2){let n=Math.max(-1,Math.min(1,e[r]));t.setInt16(s,n<0?n*32768:n*32767,!0)}return a}static mergeBuffers(e,a){let t=new Uint8Array(e.byteLength+a.byteLength);return t.set(new Uint8Array(e),0),t.set(new Uint8Array(a),e.byteLength),t.buffer}_packData(e,a){return[new Uint8Array([a,a>>8]),new Uint8Array([a,a>>8,a>>16,a>>24])][e]}pack(e,a){if(a?.bitsPerSample)if(a?.channels){if(!a?.data)throw new Error('Missing "data"')}else throw new Error('Missing "channels"');else throw new Error('Missing "bitsPerSample"');let{bitsPerSample:t,channels:s,data:r}=a,n=["RIFF",this._packData(1,52),"WAVE","fmt ",this._packData(1,16),this._packData(0,1),this._packData(0,s.length),this._packData(1,e),this._packData(1,e*s.length*t/8),this._packData(0,s.length*t/8),this._packData(0,t),"data",this._packData(1,s[0].length*s.length*t/8),r],o=new Blob(n,{type:"audio/mpeg"}),l=URL.createObjectURL(o);return{blob:o,url:l,channelCount:s.length,sampleRate:e,duration:r.byteLength/(s.length*e*2)}}};globalThis.WavPacker=m;var R=[4186.01,4434.92,4698.63,4978.03,5274.04,5587.65,5919.91,6271.93,6644.88,7040,7458.62,7902.13],F=["C","C#","D","D#","E","F","F#","G","G#","A","A#","B"],d=[],A=[];for(let f=1;f<=8;f++)for(let e=0;e<R.length;e++){let a=R[e];d.push(a/Math.pow(2,8-f)),A.push(F[e]+f)}var v=[32,2e3],B=d.filter((f,e)=>d[e]>v[0]&&d[e]<v[1]),C=A.filter((f,e)=>d[e]>v[0]&&d[e]<v[1]);var g=class f{static getFrequencies(e,a,t,s="frequency",r=-100,n=-30){t||(t=new Float32Array(e.frequencyBinCount),e.getFloatFrequencyData(t));let o=a/2,l=1/t.length*o,u,i,c;if(s==="music"||s==="voice"){let p=s==="voice"?B:d,h=Array(p.length).fill(r);for(let w=0;w<t.length;w++){let I=w*l,P=t[w];for(let k=p.length-1;k>=0;k--)if(I>p[k]){h[k]=Math.max(h[k],P);break}}u=h,i=s==="voice"?B:d,c=s==="voice"?C:A}else u=Array.from(t),i=u.map((p,h)=>l*h),c=i.map(p=>`${p.toFixed(2)} Hz`);let y=u.map(p=>Math.max(0,Math.min((p-r)/(n-r),1)));return{values:new Float32Array(y),frequencies:i,labels:c}}constructor(e,a=null){if(this.fftResults=[],a){let{length:t,sampleRate:s}=a,r=new OfflineAudioContext({length:t,sampleRate:s}),n=r.createBufferSource();n.buffer=a;let o=r.createAnalyser();o.fftSize=8192,o.smoothingTimeConstant=.1,n.connect(o);let l=1/60,u=t/s,i=c=>{let y=l*c;y<u&&r.suspend(y).then(()=>{let b=new Float32Array(o.frequencyBinCount);o.getFloatFrequencyData(b),this.fftResults.push(b),i(c+1)}),c===1?r.startRendering():r.resume()};n.start(0),i(1),this.audio=e,this.context=r,this.analyser=o,this.sampleRate=s,this.audioBuffer=a}else{let t=new AudioContext,s=t.createMediaElementSource(e),r=t.createAnalyser();r.fftSize=8192,r.smoothingTimeConstant=.1,s.connect(r),r.connect(t.destination),this.audio=e,this.context=t,this.analyser=r,this.sampleRate=this.context.sampleRate,this.audioBuffer=null}}getFrequencies(e="frequency",a=-100,t=-30){let s=null;if(this.audioBuffer&&this.fftResults.length){let r=this.audio.currentTime/this.audio.duration,n=Math.min(r*this.fftResults.length|0,this.fftResults.length-1);s=this.fftResults[n]}return f.getFrequencies(this.analyser,this.sampleRate,s,e,a,t)}async resumeIfSuspended(){return this.context.state==="suspended"&&await this.context.resume(),!0}};globalThis.AudioAnalysis=g;var T=`
// StreamProcessor

class StreamProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.hasStarted = false;
    this.hasInterrupted = false;
    this.outputBuffers = [];
    this.bufferLength = 128;
    this.writeTrackId = null;
    
    // configuration
    this.playbackRateMin = 1.0;
    this.playbackRateMax = 1.0;
    this.playbackRateAffordance = 0.2;
    this.playbackSmoothing = 0.9;
    this.playbackSkipDigitalSilence = true;
    this.playbackMinBuffers = 16; // 16 * 128 samples @ 24kHz ~ 85ms (2 server frames)
    
    // state
    this.playbackRate = 1.0;
    this.playbackOutputOffset = 0;

    // recording
    this.playbackRecord = false;
    this.playbackAudioChunks = [];

    this.isInPlayback = false;
    
    this.trackSampleOffsets = {};
    this.port.onmessage = (event) => {
      if (event.data) {
        const payload = event.data;
        if (payload.event === 'write') {
          const int16Array = payload.buffer;
          const float32Array = new Float32Array(int16Array.length);
          for (let i = 0; i < int16Array.length; i++) {
            float32Array[i] = int16Array[i] / 0x8000; // Convert Int16 to Float32
          }
          this.writeTrackId = payload.trackId;
          this.writeData(float32Array, payload.trackId);
        } else if (
          payload.event === 'offset' ||
          payload.event === 'interrupt'
        ) {
          const requestId = payload.requestId;
          const trackId = payload.trackId || this.writeTrackId;
          const offset = this.trackSampleOffsets[trackId] || 0;
          this.port.postMessage({
            event: 'offset',
            requestId,
            trackId,
            offset,
            audio: this.floatTo16BitPCM(this.mergeAudioData(this.playbackAudioChunks))
          });
          if (payload.event === 'interrupt') {
            this.hasInterrupted = true;
          }
        } else if (payload.event === 'configure') {
          const config = {
            playbackMinBuffers: this.playbackMinBuffers,
            playbackRateMin: this.playbackRateMin,
            playbackRateMax: this.playbackRateMax,
            playbackRateAffordance: this.playbackRateAffordance,
            playbackSmoothing: this.playbackSmoothing,
            playbackSkipDigitalSilence: this.playbackSkipDigitalSilence,
            ...payload.config,
          };

          this.playbackMinBuffers = config.playbackMinBuffers;
          this.playbackRateMin = config.playbackRateMin;
          this.playbackRateMax = config.playbackRateMax;
          this.playbackRateAffordance = config.playbackRateAffordance;
          this.playbackSmoothing = config.playbackSmoothing;
          this.playbackSkipDigitalSilence = config.playbackSkipDigitalSilence;
        } else {
          throw new Error('Unhandled event: ' + payload.event);
        }
      }
    };
  }

  writeData(float32Array, trackId = null) {
    let isSilence = true;
    for (let i = 0; i < float32Array.length; ++i) {
      if (float32Array[i] !== 0) {
        isSilence = false;
        break;
      }
    }

    this.outputBuffers.push({ trackId, buffer: float32Array, isSilence: isSilence });

    // this.port.postMessage({ event: 'log', data: '[worker] Consumed ' + float32Array.length + ' samples (silence: ' + isSilence + ')' });
    return true;
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];
    const outputChannelData = output[0];
    const outputBuffers = this.outputBuffers;

    if (this.hasInterrupted) {
      this.port.postMessage({ event: 'stop' });
      return false;
    } else {
      let samplesRead = 0;
      let samplesMoved = 0;
      let samplesWritten = 0

      if (outputBuffers.length > 0) {
        const outputChanneDataSampledNeeded = outputChannelData.length;
        const serverSamplesTarget = this.playbackMinBuffers * this.bufferLength;
        
        // determine if we should consume the output buffer        
        let totalSamples = -this.playbackOutputOffset;
        let consumableSamples = totalSamples;
        let shouldConsumeBuffer = false;
        
        if (this.playbackSkipDigitalSilence) {
          // count total buffered after initial non-silence buffer
          for (let i = 0; i < outputBuffers.length; ++i) {
            const { buffer, isSilence } = outputBuffers[i];
            
            totalSamples += buffer.length;
            // consider a sample as consumable if we are in or entering playback or if it is non-silence
            if (this.isInPlayback || consumableSamples > 0 || !isSilence) {
              consumableSamples += buffer.length;
            }
          }
          
          // consume samples only if we are already in playback or we've buffered enough
          shouldConsumeBuffer = this.isInPlayback || consumableSamples >= serverSamplesTarget;
        } else {
          for (let i = 0; i < outputBuffers.length; ++i) {
            consumableSamples += outputBuffers[i].buffer.length;
          }
          totalSamples = consumableSamples;
          
          // start continuous consumption once initial buffering is met
          shouldConsumeBuffer = this.hasStarted || consumableSamples >= serverSamplesTarget;
        }

        if (shouldConsumeBuffer && consumableSamples > 0) {
          // apply playback rate to determine how many samples to consume
          const playbackRateTarget = this.determinePlaybackRate(consumableSamples, serverSamplesTarget);
          this.playbackRate = this.playbackRate * this.playbackSmoothing + playbackRateTarget * (1 - this.playbackSmoothing);
  
          const outputBufferSamplesNeeded = Math.floor(outputChanneDataSampledNeeded * this.playbackRate);
          const outputBuffer = new Float32Array(outputBufferSamplesNeeded);

          // this.port.postMessage({ event: 'log', data: '[worker] Consuming ' + outputBufferSamplesNeeded + ' of ' + consumableSamples + ' samples (total: ' + totalSamples + ') @ ' + this.playbackRate });

          // read the necessary (or as many as available) samples from the outputBuffers
          let outputBufferIndex = 0;
          let outputBufferOffset = this.playbackOutputOffset;
          let outputTrackId = null;
          while (outputBufferIndex < outputBuffers.length) {
            const { trackId, buffer, isSilence } = outputBuffers[outputBufferIndex];

            outputTrackId = trackId;

            // skip full buffers of silence (if enabled)
            if (this.playbackSkipDigitalSilence) {
              if (isSilence && outputBufferOffset === 0) {
                samplesMoved += buffer.length;
                // advance buffer
                outputBufferIndex++;
                continue;
              }
            }

            // read samples from the buffer
            for (let j = outputBufferOffset; j < buffer.length && samplesRead < outputBufferSamplesNeeded; ++j) {
              outputBuffer[samplesRead++] = buffer[j];
              samplesMoved++;
              
              // advance output buffer
              if (j === buffer.length - 1) {
                outputBufferOffset = 0;
                outputBufferIndex++;
              } else {
                outputBufferOffset++;
              }
            }

            // done if read enough samples
            if (samplesRead === outputBufferSamplesNeeded) {
              break;
            }
          }

          // done if no samples
          if (samplesRead > 0) {
            // apply playback rate to output buffer
            const resampledBuffer = this.resampleAudioData(outputBuffer, outputChanneDataSampledNeeded);
            
            // write the resampled buffer to the output channel
            for (let i = 0; i < resampledBuffer.length && samplesWritten < outputChanneDataSampledNeeded; ++i) {
              outputChannelData[samplesWritten++] = resampledBuffer[i];
            }

            // update output buffers
            this.outputBuffers = this.outputBuffers.slice(outputBufferIndex);
            this.playbackOutputOffset = outputBufferOffset;
          
            if (outputTrackId) {
              this.trackSampleOffsets[outputTrackId] =
                this.trackSampleOffsets[outputTrackId] || 0;
              this.trackSampleOffsets[outputTrackId] += resampledBuffer.length;
            }
          }
        }
      }

      if (samplesMoved > 0) {
        this.hasStarted = true;

        // post audio playback timestamp
        this.port.postMessage({
          event: 'audio',
          data: samplesMoved,
          timestamp_ms: Date.now(),
        });

        // append audio chunk and merge if necessary
        if (this.playbackEnableRecording) {
          this.playbackAudioChunks.push(outputChannelData.slice(0));
          if (this.playbackAudioChunks.length > 64) {
            this.playbackAudioChunks = [this.mergeAudioData(this.playbackAudioChunks)];
          }
        }
      }

      if (samplesWritten > 0) {
        this.isInPlayback = true;
      } else {
        this.isInPlayback = false;
      }

      return true;
    }
  }

  // utility

  determinePlaybackRate(availableSamples, targetSamples) {
    let playbackRate = 1.0;
    if (this.playbackRateMin < this.playbackRateMax) {
      // adjust playback rate based on how far we are from the target (with affordance)
      const samplesDelta = availableSamples - targetSamples;
      if (Math.abs(samplesDelta) > this.playbackRateAffordance * targetSamples) {
        if (samplesDelta <= 0) {
          // slow down
          playbackRate = 1.0 + Math.max(-0.975, samplesDelta / targetSamples);
        } else {
          // speed up
          playbackRate = 1.0 / (1.0 - Math.min(0.975, samplesDelta / targetSamples));
        }
      }
      
      playbackRate = Math.min(this.playbackRateMax, Math.max(this.playbackRateMin, playbackRate));
    }

    return playbackRate;
  }

  resampleAudioData(float32Array, targetSamples) {
    if (targetSamples === float32Array.length) {
      return float32Array;
    }

    // Apply playback rate by resampling into a new buffer
    const resampledBuffer = new Float32Array(targetSamples);
    const playbackRate = float32Array.length / targetSamples;

    for (let i = 0; i < targetSamples; ++i) {
      const originalIndex = i * playbackRate;
      const start = Math.floor(originalIndex);
      const end = Math.ceil(originalIndex);

      if (start === end || end >= float32Array.length) {
        // If the start and end are the same or out of bounds, just use the start value
        resampledBuffer[i] = float32Array[start];
      } else {
        // Linear interpolation between two samples
        const ratio = originalIndex - start;
        resampledBuffer[i] = float32Array[start] * (1 - ratio) + float32Array[end] * ratio;
      }
    }
    
    return resampledBuffer;
  }

  mergeAudioData(float32Arrays) {
    let samples = 0;
    for (let i = 0; i < float32Arrays.length; ++i) {
      samples += float32Arrays[i].length;
    }

    const merged = new Float32Array(samples);
    let offset = 0;
    for (let i = 0; i < float32Arrays.length; ++i) {
      const chunk = float32Arrays[i];
      merged.set(chunk, offset);
      offset += chunk.length;
    }
      
    return merged;
  }

  floatTo16BitPCM(float32Array) {
    const buffer = new ArrayBuffer(float32Array.length * 2);
    const view = new DataView(buffer);
    let offset = 0;
    for (let i = 0; i < float32Array.length; i++, offset += 2) {
      let s = Math.max(-1, Math.min(1, float32Array[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }

    return buffer;
  }
}

registerProcessor('stream_processor', StreamProcessor);
`,_=new Blob([T],{type:"application/javascript"}),q=URL.createObjectURL(_),M=q;var S=class{constructor({sampleRate:e=44100}={}){this.scriptSrc=M,this.sampleRate=e,this.context=null,this.stream=null,this.analyser=null,this.trackSampleOffsets={},this.interruptedTrackIds={},this._audioProcessor=()=>{}}async connect(e){this.context=new AudioContext({sampleRate:this.sampleRate}),this._audioProcessor=e,this.context.state==="suspended"&&await this.context.resume();try{await this.context.audioWorklet.addModule(this.scriptSrc)}catch(t){throw console.error(t),new Error(`Could not add audioWorklet module: ${this.scriptSrc}`)}let a=this.context.createAnalyser();return a.fftSize=8192,a.smoothingTimeConstant=.1,this.analyser=a,!0}getFrequencies(e="frequency",a=-100,t=-30){if(!this.analyser)throw new Error("Not connected, please call .connect() first");return g.getFrequencies(this.analyser,this.sampleRate,null,e,a,t)}_start(){let e=new AudioWorkletNode(this.context,"stream_processor");return e.connect(this.context.destination),e.port.onmessage=a=>{let{event:t,data:s,timestamp_ms:r}=a.data;if(t==="audio")this._audioProcessor(s,r);else if(t==="stop")e.disconnect(),this.stream=null;else if(t==="offset"){let{requestId:n,trackId:o,offset:l,audio:u}=a.data,i=l/this.sampleRate;this.trackSampleOffsets[n]={trackId:o,offset:l,currentTime:i,audio:u}}else t==="log"&&console.log(s)},this.analyser.disconnect(),e.connect(this.analyser),this.stream=e,!0}configure(e){this.stream.port.postMessage({event:"configure",config:e})}add16BitPCM(e,a="default"){if(typeof a!="string")throw new Error("trackId must be a string");if(this.interruptedTrackIds[a])return;this.stream||this._start();let t;if(e instanceof Int16Array)t=e;else if(e instanceof ArrayBuffer)t=new Int16Array(e);else throw new Error("argument must be Int16Array or ArrayBuffer");return this.stream.port.postMessage({event:"write",buffer:t,trackId:a}),t}async getTrackSampleOffset(e=!1){if(!this.stream)return null;let a=crypto.randomUUID();this.stream.port.postMessage({event:e?"interrupt":"offset",requestId:a});let t;for(;!t;)t=this.trackSampleOffsets[a],await new Promise(r=>setTimeout(()=>r(),1));let{trackId:s}=t;return e&&s&&(this.interruptedTrackIds[s]=!0),t}async interrupt(){let e=await this.getTrackSampleOffset(!0);return e?e.audio:null}};globalThis.WavStreamPlayer=S;var E=`
class AudioProcessor extends AudioWorkletProcessor {

  constructor() {
    super();
    this.port.onmessage = this.receive.bind(this);
    this.initialize();
  }

  initialize() {
    this.foundAudio = false;
    this.recording = false;
    this.chunks = [];
  }

  /**
   * Concatenates sampled chunks into channels
   * Format is chunk[Left[], Right[]]
   */
  readChannelData(chunks, channel = -1, maxChannels = 9) {
    let channelLimit;
    if (channel !== -1) {
      if (chunks[0] && chunks[0].length - 1 < channel) {
        throw new Error(
          \`Channel \${channel} out of range: max \${chunks[0].length}\`
        );
      }
      channelLimit = channel + 1;
    } else {
      channel = 0;
      channelLimit = Math.min(chunks[0] ? chunks[0].length : 1, maxChannels);
    }
    const channels = [];
    for (let n = channel; n < channelLimit; n++) {
      const length = chunks.reduce((sum, chunk) => {
        return sum + chunk[n].length;
      }, 0);
      const buffers = chunks.map((chunk) => chunk[n]);
      const result = new Float32Array(length);
      let offset = 0;
      for (let i = 0; i < buffers.length; i++) {
        result.set(buffers[i], offset);
        offset += buffers[i].length;
      }
      channels[n] = result;
    }
    return channels;
  }

  /**
   * Combines parallel audio data into correct format,
   * channels[Left[], Right[]] to float32Array[LRLRLRLR...]
   */
  formatAudioData(channels) {
    if (channels.length === 1) {
      // Simple case is only one channel
      const float32Array = channels[0].slice();
      const meanValues = channels[0].slice();
      return { float32Array, meanValues };
    } else {
      const float32Array = new Float32Array(
        channels[0].length * channels.length
      );
      const meanValues = new Float32Array(channels[0].length);
      for (let i = 0; i < channels[0].length; i++) {
        const offset = i * channels.length;
        let meanValue = 0;
        for (let n = 0; n < channels.length; n++) {
          float32Array[offset + n] = channels[n][i];
          meanValue += channels[n][i];
        }
        meanValues[i] = meanValue / channels.length;
      }
      return { float32Array, meanValues };
    }
  }

  /**
   * Converts 32-bit float data to 16-bit integers
   */
  floatTo16BitPCM(float32Array) {
    const buffer = new ArrayBuffer(float32Array.length * 2);
    const view = new DataView(buffer);
    let offset = 0;
    for (let i = 0; i < float32Array.length; i++, offset += 2) {
      let s = Math.max(-1, Math.min(1, float32Array[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
    }
    return buffer;
  }

  /**
   * Retrieves the most recent amplitude values from the audio stream
   * @param {number} channel
   */
  getValues(channel = -1) {
    const channels = this.readChannelData(this.chunks, channel);
    const { meanValues } = this.formatAudioData(channels);
    return { meanValues, channels };
  }

  /**
   * Exports chunks as an audio/wav file
   */
  export() {
    const channels = this.readChannelData(this.chunks);
    const { float32Array, meanValues } = this.formatAudioData(channels);
    const audioData = this.floatTo16BitPCM(float32Array);
    const monoData = this.floatTo16BitPCM(meanValues);
    return {
      audio: {
        bitsPerSample: 16,
        channels: channels,
        data: audioData,
        mono: monoData,
      },
    };
  }

  receive(e) {
    const { event, id } = e.data;
    let receiptData = {};
    switch (event) {
      case 'start':
        this.recording = true;
        break;
      case 'stop':
        this.recording = false;
        break;
      case 'clear':
        this.initialize();
        break;
      case 'export':
        receiptData = this.export();
        break;
      case 'read':
        receiptData = this.getValues();
        break;
      default:
        break;
    }
    // Always send back receipt
    this.port.postMessage({ event: 'receipt', id, data: receiptData });
  }

  sendChunk(chunk) {
    const timestamp_ms = Date.now();
    const channels = this.readChannelData([chunk]);
    const { float32Array, meanValues } = this.formatAudioData(channels);
    const rawAudioData = this.floatTo16BitPCM(float32Array);
    const monoAudioData = this.floatTo16BitPCM(meanValues);
    this.port.postMessage({
      event: 'chunk',
      data: {
        mono: monoAudioData,
        raw: rawAudioData,
      },
      timestamp_ms,
    });
  }

  process(inputList, outputList, parameters) {    
    // Copy input to output (e.g. speakers)
    // Note that this creates choppy sounds with Mac products
    const sourceLimit = Math.min(inputList.length, outputList.length);
    for (let inputNum = 0; inputNum < sourceLimit; inputNum++) {
      const input = inputList[inputNum];
      const output = outputList[inputNum];
      const channelCount = Math.min(input.length, output.length);
      for (let channelNum = 0; channelNum < channelCount; channelNum++) {
        input[channelNum].forEach((sample, i) => {
          output[channelNum][i] = sample;
        });
      }
    }
    const inputs = inputList[0];
    // There's latency at the beginning of a stream before recording starts
    // Make sure we actually receive audio data before we start storing chunks
    let sliceIndex = 0;
    if (!this.foundAudio) {
      for (const channel of inputs) {
        sliceIndex = 0; // reset for each channel
        if (this.foundAudio) {
          break;
        }
        if (channel) {
          for (const value of channel) {
            if (value !== 0) {
              // find only one non-zero entry in any channel
              this.foundAudio = true;
              break;
            } else {
              sliceIndex++;
            }
          }
        }
      }
    }
    if (inputs && inputs[0] && this.foundAudio && this.recording) {
      // We need to copy the TypedArray, because the \`process\`
      // internals will reuse the same buffer to hold each input
      const chunk = inputs.map((input) => input.slice(sliceIndex));
      this.chunks.push(chunk);
      this.sendChunk(chunk);
    }
    return true;
  }
}

registerProcessor('audio_processor', AudioProcessor);
`,L=new Blob([E],{type:"application/javascript"}),O=URL.createObjectURL(L),D=O;var x=class{constructor({sampleRate:e=44100,outputToSpeakers:a=!1,debug:t=!1}={}){this.scriptSrc=D,this.sampleRate=e,this.outputToSpeakers=a,this.debug=!!t,this._deviceChangeCallback=null,this._devices=[],this.stream=null,this.processor=null,this.source=null,this.node=null,this.recording=!1,this._lastEventId=0,this.eventReceipts={},this.eventTimeout=5e3,this._chunkProcessor=()=>{},this._chunkProcessorSize=void 0,this._chunkProcessorBuffer={raw:new ArrayBuffer(0),mono:new ArrayBuffer(0)}}static async decode(e,a=44100,t=-1){let s=new AudioContext({sampleRate:a}),r,n;if(e instanceof Blob){if(t!==-1)throw new Error('Can not specify "fromSampleRate" when reading from Blob');n=e,r=await n.arrayBuffer()}else if(e instanceof ArrayBuffer){if(t!==-1)throw new Error('Can not specify "fromSampleRate" when reading from ArrayBuffer');r=e,n=new Blob([r],{type:"audio/wav"})}else{let i,c;if(e instanceof Int16Array){c=e,i=new Float32Array(e.length);for(let h=0;h<e.length;h++)i[h]=e[h]/32768}else if(e instanceof Float32Array)i=e;else if(e instanceof Array)i=new Float32Array(e);else throw new Error('"audioData" must be one of: Blob, Float32Arrray, Int16Array, ArrayBuffer, Array<number>');if(t===-1)throw new Error('Must specify "fromSampleRate" when reading from Float32Array, In16Array or Array');if(t<3e3)throw new Error('Minimum "fromSampleRate" is 3000 (3kHz)');c||(c=m.floatTo16BitPCM(i));let y={bitsPerSample:16,channels:[i],data:c};n=new m().pack(t,y).blob,r=await n.arrayBuffer()}let o=await s.decodeAudioData(r),l=o.getChannelData(0),u=URL.createObjectURL(n);return{blob:n,url:u,values:l,audioBuffer:o}}log(){return this.debug&&this.log(...arguments),!0}getSampleRate(){return this.sampleRate}getStatus(){return this.processor?this.recording?"recording":"paused":"ended"}async _event(e,a={},t=null){if(t=t||this.processor,!t)throw new Error("Can not send events without recording first");let s={event:e,id:this._lastEventId++,data:a};t.port.postMessage(s);let r=new Date().valueOf();for(;!this.eventReceipts[s.id];){if(new Date().valueOf()-r>this.eventTimeout)throw new Error(`Timeout waiting for "${e}" event`);await new Promise(o=>setTimeout(()=>o(!0),1))}let n=this.eventReceipts[s.id];return delete this.eventReceipts[s.id],n}listenForDeviceChange(e){if(e===null&&this._deviceChangeCallback)navigator.mediaDevices.removeEventListener("devicechange",this._deviceChangeCallback),this._deviceChangeCallback=null;else if(e!==null){let a=0,t=[],s=n=>n.map(o=>o.deviceId).sort().join(","),r=async()=>{let n=++a,o=await this.listDevices();n===a&&s(t)!==s(o)&&(t=o,e(o.slice()))};navigator.mediaDevices.addEventListener("devicechange",r),r(),this._deviceChangeCallback=r}return!0}async requestPermission(){let e=await navigator.permissions.query({name:"microphone"});if(e.state==="denied")window.alert("You must grant microphone access to use this feature.");else if(e.state==="prompt")try{(await navigator.mediaDevices.getUserMedia({audio:!0})).getTracks().forEach(s=>s.stop())}catch{window.alert("You must grant microphone access to use this feature.")}return!0}async listDevices(){if(!navigator.mediaDevices||!("enumerateDevices"in navigator.mediaDevices))throw new Error("Could not request user devices");await this.requestPermission();let a=(await navigator.mediaDevices.enumerateDevices()).filter(r=>r.kind==="audioinput"),t=a.findIndex(r=>r.deviceId==="default"),s=[];if(t!==-1){let r=a.splice(t,1)[0],n=a.findIndex(o=>o.groupId===r.groupId);n!==-1&&(r=a.splice(n,1)[0]),r.default=!0,s.push(r)}return s.concat(a)}async begin(e){if(this.processor)throw new Error("Already connected: please call .end() to start a new session");if(!navigator.mediaDevices||!("getUserMedia"in navigator.mediaDevices))throw new Error("Could not request user media");try{let o={audio:{echoCancellation:!0}};e&&(o.audio.deviceId={exact:e}),this.stream=await navigator.mediaDevices.getUserMedia(o);let l=this.stream.getAudioTracks()[0];console.log("track-settings",l.getSettings())}catch{throw new Error("Could not start media stream")}let a=new AudioContext({sampleRate:this.sampleRate}),t=a.createMediaStreamSource(this.stream);try{await a.audioWorklet.addModule(this.scriptSrc)}catch(o){throw console.error(o),new Error(`Could not add audioWorklet module: ${this.scriptSrc}`)}let s=new AudioWorkletNode(a,"audio_processor");s.port.onmessage=o=>{let{event:l,id:u,data:i,timestamp_ms:c}=o.data;if(l==="receipt")this.eventReceipts[u]=i;else if(l==="chunk"){if(this._chunkProcessorSize)throw new Error("deprecated - chunkSize must be 0 - do not use buffering");this._chunkProcessor(i,c)}};let r=t.connect(s),n=a.createAnalyser();return n.fftSize=8192,n.smoothingTimeConstant=.1,r.connect(n),this.outputToSpeakers&&(console.warn(`Warning: Output to speakers may affect sound quality,
especially due to system audio feedback preventative measures.
use only for debugging`),n.connect(a.destination)),this.source=t,this.node=r,this.analyser=n,this.processor=s,!0}getFrequencies(e="frequency",a=-100,t=-30){if(!this.processor)throw new Error("Session ended: please call .begin() first");return g.getFrequencies(this.analyser,this.sampleRate,null,e,a,t)}async pause(){if(this.processor){if(!this.recording)throw new Error("Already paused: please call .record() first")}else throw new Error("Session ended: please call .begin() first");return this.log("Pausing ..."),await this._event("stop"),this.recording=!1,!0}async record(e=()=>{}){if(this.processor){if(this.recording)throw new Error("Already recording: please call .pause() first");if(typeof e!="function")throw new Error("chunkProcessor must be a function")}else throw new Error("Session ended: please call .begin() first");return this._chunkProcessor=e,this._chunkProcessorSize=0,this._chunkProcessorBuffer={raw:new ArrayBuffer(0),mono:new ArrayBuffer(0)},this.log("Recording ..."),await this._event("start"),this.recording=!0,!0}async clear(){if(!this.processor)throw new Error("Session ended: please call .begin() first");return await this._event("clear"),!0}async read(){if(!this.processor)throw new Error("Session ended: please call .begin() first");return this.log("Reading ..."),await this._event("read")}async save(e=!1){if(!this.processor)throw new Error("Session ended: please call .begin() first");if(!e&&this.recording)throw new Error("Currently recording: please call .pause() first, or call .save(true) to force");this.log("Exporting ...");let a=await this._event("export");return new m().pack(this.sampleRate,a.audio)}async end(){if(!this.processor)throw new Error("Session ended: please call .begin() first");let e=this.processor;this.log("Stopping ..."),await this._event("stop"),this.recording=!1,this.stream.getTracks().forEach(s=>s.stop()),this.log("Exporting ...");let t=await this._event("export",{},e);return this.processor.disconnect(),this.source.disconnect(),this.node.disconnect(),this.analyser.disconnect(),this.stream=null,this.processor=null,this.source=null,this.node=null,t.audio.mono}async quit(){return this.listenForDeviceChange(null),this.processor&&await this.end(),!0}};globalThis.WavRecorder=x;})();
